@article{foerster2016learning,
  title={Learning to communicate with deep multi-agent reinforcement learning},
  author={Foerster, Jakob and Assael, Ioannis Alexandros and De Freitas, Nando and Whiteson, Shimon},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}

@article{zhu2024survey,
  title={A survey of multi-agent deep reinforcement learning with communication},
  author={Zhu, Changxi and Dastani, Mehdi and Wang, Shihan},
  journal={Autonomous Agents and Multi-Agent Systems},
  volume={38},
  number={1},
  pages={4},
  year={2024},
  publisher={Springer}
}

@misc{silver2015rl,
  author       = {David Silver},
  title        = {Lectures on Reinforcement Learning},
  howpublished = {\textsc{url:}~\url{https://www.davidsilver.uk/teaching/}},
  year         = {2015}
}

@book{szepesvari2022algorithms,
  title = {Algorithms for Reinforcement Learning},
  author = {Szepesv{\'a}ri, Csaba},
  year = {2022},
  publisher = {Springer Nature}
}

@book{albrecht2024marl,
  author = {Stefano V. Albrecht and Filippos Christianos and Lukas Sch\"afer},
  title = {Multi-Agent Reinforcement Learning: Foundations and Modern Approaches},
  publisher = {MIT Press},
  year = {2024},
  url = {https://www.marl-book.com}
}

@article{kwiatkowski2024gymnasium,
      title={Gymnasium: A Standard Interface for Reinforcement Learning Environments},
      author={Ariel Kwiatkowski and Mark Towers and Jordan Terry and John U. Balis and Gianluca De Cola and Tristan Deleu and Manuel Goulão and Andreas Kallinteris and Markus Krimmel and Arjun KG and Rodrigo Perez-Vicente and Andrea Pierré and Sander Schulhoff and Jun Jet Tai and Hannah Tan and Omar G. Younis},
      year={2024},
      eprint={2407.17032},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2407.17032},
}

@misc{moerland2022model,
  title = {Model-Based {{Reinforcement Learning}}: {{A Survey}}},
  shorttitle = {Model-Based {{Reinforcement Learning}}},
  author = {Moerland, Thomas M. and Broekens, Joost and Plaat, Aske and Jonker, Catholijn M.},
  year = {2022},
  month = mar,
  number = {arXiv:2006.16712},
  eprint = {2006.16712},
  publisher = {arXiv},
  urldate = {2024-10-22},
  abstract = {Sequential decision making, commonly formalized as Markov Decision Process (MDP) optimization, is a important challenge in artificial intelligence. Two key approaches to this problem are reinforcement learning (RL) and planning. This paper presents a survey of the integration of both fields, better known as model-based reinforcement learning. Model-based RL has two main steps. First, we systematically cover approaches to dynamics model learning, including challenges like dealing with stochasticity, uncertainty, partial observability, and temporal abstraction. Second, we present a systematic categorization of planning-learning integration, including aspects like: where to start planning, what budgets to allocate to planning and real data collection, how to plan, and how to integrate planning in the learning and acting loop. After these two sections, we also discuss implicit model-based RL as an end-to-end alternative for model learning and planning, and we cover the potential benefits of model-based RL. Along the way, the survey also draws connections to several related RL fields, like hierarchical RL and transfer learning. Altogether, the survey presents a broad conceptual overview of the combination of planning and learning for MDP optimization.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/jackmontgomery/Zotero/storage/WI7XHDSG/Moerland et al. - 2022 - Model-based Reinforcement Learning A Survey.pdf;/Users/jackmontgomery/Zotero/storage/YQK8BLTJ/2006.html}
}

@article{mnih2013playing,
  title={Playing atari with deep reinforcement learning},
  author={Mnih, Volodymyr},
  journal={arXiv preprint arXiv:1312.5602},
  year={2013}
}

@inproceedings{tsitsiklis1996temporal,
 author = {Tsitsiklis, John and Van Roy, Benjamin},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 pages = {},
 publisher = {MIT Press},
 title = {Analysis of Temporal-Diffference Learning with Function Approximation},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/file/e00406144c1e7e35240afed70f34166a-Paper.pdf},
 volume = {9},
 year = {1996}
}

@inproceedings{hausknecht2015deep,
  title={Deep recurrent q-learning for partially observable mdps},
  author={Hausknecht, Matthew and Stone, Peter},
  booktitle={2015 aaai fall symposium series},
  year={2015}
}

@article{sukhbaatar2016commnet,
  title={Learning multiagent communication with backpropagation},
  author={Sukhbaatar, Sainbayar and Fergus, Rob and others},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}

@inproceedings{
singh2018ic3net,
title={Individualized Controlled Continuous Communication Model for Multiagent Cooperative and Competitive Tasks},
author={Amanpreet Singh and Tushar Jain and Sainbayar Sukhbaatar},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=rye7knCqK7},
}

@article{williams1992simple,
  title={Simple statistical gradient-following algorithms for connectionist reinforcement learning},
  author={Williams, Ronald J},
  journal={Machine learning},
  volume={8},
  pages={229--256},
  year={1992},
  publisher={Springer}
}

@ARTICLE{chuster1997Bidirectional,
  author={Schuster, M. and Paliwal, K.K.},
  journal={IEEE Transactions on Signal Processing}, 
  title={Bidirectional recurrent neural networks}, 
  year={1997},
  volume={45},
  number={11},
  pages={2673-2681},
  keywords={Recurrent neural networks;Artificial neural networks;Training data;Databases;Probability;Shape;Parameter estimation;Speech recognition;Control systems;Telecommunication control},
  doi={10.1109/78.650093}
 }

@article{peng2017bicnet,
author = {Peng, Peng and Yuan, Quan and Wen, Ying and Yang, Yaodong and Tang, Zhenkun and Long, Haitao and Wang, Jun},
year = {2017},
month = {03},
pages = {},
title = {Multiagent Bidirectionally-Coordinated Nets for Learning to Play StarCraft Combat Games},
doi = {10.48550/arXiv.1703.10069}
}

@inproceedings{chu2020NeurComm,
title={Multi-agent Reinforcement Learning for Networked System Control},
author={Tianshu Chu and Sandeep Chinchali and Sachin Katti},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=Syx7A3NFvH}
}


@techreport{wu2002prisoners,
  author = {W. Wu},
  title = {100 prisoners and a lightbulb},
  institution = {OCF, UC Berkeley},
  year = {2002},
  type = {Technical Report}
}

@misc{gupta2022HAMMER,
  title = {{{HAMMER}}: {{Multi-Level Coordination}} of {{Reinforcement Learning Agents}} via {{Learned Messaging}}},
  shorttitle = {{{HAMMER}}},
  author = {Gupta, Nikunj and Srinivasaraghavan, G. and Mohalik, Swarup Kumar and Kumar, Nishant and Taylor, Matthew E.},
  year = {2022},
  month = dec,
  number = {arXiv:2102.00824},
  eprint = {2102.00824},
  publisher = {arXiv},
  urldate = {2024-10-30},
  abstract = {Cooperative multi-agent reinforcement learning (MARL) has achieved significant results, most notably by leveraging the representation-learning abilities of deep neural networks. However, large centralized approaches quickly become infeasible as the number of agents scale, and fully decentralized approaches can miss important opportunities for information sharing and coordination. Furthermore, not all agents are equal -- in some cases, individual agents may not even have the ability to send communication to other agents or explicitly model other agents. This paper considers the case where there is a single, powerful, {\textbackslash}emph\{central agent\} that can observe the entire observation space, and there are multiple, low-powered {\textbackslash}emph\{local agents\} that can only receive local observations and are not able to communicate with each other. The central agent's job is to learn what message needs to be sent to different local agents based on the global observations, not by centrally solving the entire problem and sending action commands, but by determining what additional information an individual agent should receive so that it can make a better decision. In this work we present our MARL algorithm {\textbackslash}algo, describe where it would be most applicable, and implement it in the cooperative navigation and multi-agent walker domains. Empirical results show that 1) learned communication does indeed improve system performance, 2) results generalize to heterogeneous local agents, and 3) results generalize to different reward structures.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Multiagent Systems},
  file = {/Users/jackmontgomery/Zotero/storage/U3CHA6MY/Gupta et al. - 2022 - HAMMER Multi-Level Coordination of Reinforcement Learning Agents via Learned Messaging.pdf;/Users/jackmontgomery/Zotero/storage/UWFQH7NW/2102.html}
}

@article{schulman2017proximal,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}

@inproceedings{sutton1999policy,
  title = {Policy {{Gradient Methods}} for {{Reinforcement Learning}} with {{Function Approximation}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Sutton, Richard S and McAllester, David and Singh, Satinder and Mansour, Yishay},
  year = {1999},
  volume = {12},
  publisher = {MIT Press},
  urldate = {2024-10-01},
  abstract = {Function  approximation  is  essential  to  reinforcement  learning,  but  the standard approach of approximating a  value function and deter(cid:173) mining  a  policy  from  it  has so  far  proven theoretically  intractable.  In this paper we explore an alternative approach in which the policy  is explicitly represented by its own function approximator,  indepen(cid:173) dent of the value function,  and is  updated according to the gradient  of expected reward with respect to the policy parameters.  Williams's  REINFORCE method and actor-critic methods are examples of this  approach.  Our  main  new  result  is  to  show  that  the  gradient  can  be  written  in  a  form  suitable  for  estimation  from  experience  aided  by  an  approximate  action-value  or  advantage  function.  Using  this  result,  we  prove for  the first  time that a  version  of policy  iteration  with arbitrary differentiable function approximation is convergent to  a  locally optimal policy.},
  file = {/Users/jackmontgomery/Zotero/storage/UNJUA4SN/Sutton et al. - 1999 - Policy Gradient Methods for Reinforcement Learning with Function Approximation.pdf}
}

@article{schulman2015trust,
  title={Trust Region Policy Optimization},
  author={Schulman, John},
  journal={arXiv preprint arXiv:1502.05477},
  year={2015}
}

@book{sutton2018reinforcement,
  added-at = {2019-07-13T10:11:53.000+0200},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  biburl = {https://www.bibsonomy.org/bibtex/2f46601cf8b13d39d1378af0d79438b12/lanteunis},
  edition = {Second},
  interhash = {ac6b144aaec1819919a2fba9f705c852},
  intrahash = {f46601cf8b13d39d1378af0d79438b12},
  keywords = {},
  publisher = {The MIT Press},
  timestamp = {2019-07-13T10:11:53.000+0200},
  title = {Reinforcement Learning: An Introduction},
  url = {http://incompleteideas.net/book/the-book-2nd.html},
  year = {2018}
}

@article{chao2021surprising,
  author       = {Chao Yu and
                  Akash Velu and
                  Eugene Vinitsky and
                  Yu Wang and
                  Alexandre M. Bayen and
                  Yi Wu},
  title        = {The Surprising Effectiveness of {MAPPO} in Cooperative, Multi-Agent
                  Games},
  journal      = {CoRR},
  volume       = {abs/2103.01955},
  year         = {2021},
  url          = {https://arxiv.org/abs/2103.01955},
  eprinttype    = {arXiv},
  eprint       = {2103.01955},
  timestamp    = {Tue, 02 Apr 2024 15:05:11 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2103-01955.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@book{oliehoek2016concise,
  title={A concise introduction to decentralized POMDPs},
  author={Oliehoek, Frans A and Amato, Christopher and others},
  volume={1},
  year={2016},
  publisher={Springer}
}

@inproceedings{hansen2004dynamic,
  title={Dynamic programming for partially observable stochastic games},
  author={Hansen, Eric A and Bernstein, Daniel S and Zilberstein, Shlomo},
  booktitle={AAAI},
  volume={4},
  pages={709--715},
  year={2004}
}

@misc{tampuu2015multiagent,
      title={Multiagent Cooperation and Competition with Deep Reinforcement Learning}, 
      author={Ardi Tampuu and Tambet Matiisen and Dorian Kodelja and Ilya Kuzovkin and Kristjan Korjus and Juhan Aru and Jaan Aru and Raul Vicente},
      year={2015},
      eprint={1511.08779},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/1511.08779}, 
}

@inproceedings{tan1997MultiAgentRL,
  title={Multi-Agent Reinforcement Learning: Independent versus Cooperative Agents},
  author={Ming Tan},
  booktitle={International Conference on Machine Learning},
  year={1997},
  url={https://api.semanticscholar.org/CorpusID:273638739}
}
@article{witt2020independent,
  title={Is Independent Learning All You Need in the StarCraft Multi-Agent Challenge?},
  author={C. S. D. Witt and Tarun Gupta and Denys Makoviichuk and Viktor Makoviychuk and Philip H. S. Torr and Mingfei Sun and Shimon Whiteson},
  journal={ArXiv},
  year={2020},
  volume={abs/2011.09533},
  url={https://api.semanticscholar.org/CorpusID:227054146}
}

@inproceedings{gupta2017cooperative,
  title={Cooperative Multi-agent Control Using Deep Reinforcement Learning},
  author={Jayesh K. Gupta and Maxim Egorov and Mykel J. Kochenderfer},
  booktitle={AAMAS Workshops},
  year={2017},
  url={https://api.semanticscholar.org/CorpusID:9421360}
}

@inproceedings{pearl1982bayes,
author = {Pearl, Judea},
title = {Reverend bayes on inference engines: a distributed hierarchical approach},
year = {1982},
publisher = {AAAI Press},
abstract = {This paper presents generalizations of Bayes likelihood-ratio updating rule which facilitate an asynchronous propagation of the impacts of new beliefs and/or new evidence in hierarchically organized inference structures with multi-hypotheses variables. The computational scheme proposed specifies a set of belief parameters, communication messages and updating rules which guarantee that the diffusion of updated beliefs is accomplished in a single pass and complies with the tenets of Bayes calculus.},
booktitle = {Proceedings of the Second AAAI Conference on Artificial Intelligence},
pages = {133–136},
numpages = {4},
location = {Pittsburgh, Pennsylvania},
series = {AAAI'82}
}

@article{hochreiter1997long,
  author      = {Sepp Hochreiter and Jürgen Schmidhuber},
  journal     = {Neural Computation},
  title       = {Long Short-Term Memory},
  year        = {1997},
  number      = {8},
  pages       = {1735--1780},
  volume      = {9},
  optabstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
  optdoi      = {10.1162/neco.1997.9.8.1735},
  opteprint   = {http://dx.doi.org/10.1162/neco.1997.9.8.1735},
  opturl      = {http://dx.doi.org/10.1162/neco.1997.9.8.1735},
}

@ARTICLE{werbos1990backpropagation,
  author={Werbos, P.J.},
  journal={Proceedings of the IEEE}, 
  title={Backpropagation through time: what it does and how to do it}, 
  year={1990},
  volume={78},
  number={10},
  pages={1550-1560},
  keywords={Backpropagation;Artificial neural networks;Supervised learning;Pattern recognition;Neural networks;Power system modeling;Equations;Control systems;Fluid dynamics;Books},
  doi={10.1109/5.58337}
}

@misc{shavlev2016safe,
      title={Safe, Multi-Agent, Reinforcement Learning for Autonomous Driving}, 
      author={Shai Shalev-Shwartz and Shaked Shammah and Amnon Shashua},
      year={2016},
      eprint={1610.03295},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/1610.03295}, 
}


@article{vinyals2014sensor,
author = {Vinyals, M and Rodríguez-Aguilar, Juan and Cerquides, Jesús},
year = {2014},
month = {01},
pages = {},
title = {A Survey on Sensor Networks from a Multi-Agent perspective}
}

@article{kober2013reinforcement,
author = {Kober, Jens and Bagnell, J. and Peters, Jan},
year = {2013},
month = {09},
pages = {1238-1274},
title = {Reinforcement Learning in Robotics: A Survey},
volume = {32},
isbn = {978-3-642-27644-6},
journal = {The International Journal of Robotics Research},
doi = {10.1177/0278364913495721}
}

@misc{subramanian2020psychological,
      title={Reinforcement Learning and its Connections with Neuroscience and Psychology}, 
      author={Ajay Subramanian and Sharad Chitlangia and Veeky Baths},
      year={2021},
      eprint={2007.01099},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2007.01099}, 
}

@ARTICLE{wang2024deep,
  author={Wang, Xu and Wang, Sen and Liang, Xingxing and Zhao, Dawei and Huang, Jincai and Xu, Xin and Dai, Bin and Miao, Qiguang},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={Deep Reinforcement Learning: A Survey}, 
  year={2024},
  volume={35},
  number={4},
  pages={5064-5078},
  keywords={Task analysis;Mathematical models;Deep learning;Trajectory;Behavioral sciences;Q-learning;Dynamic programming;Deep learning;deep reinforcement learning (DRL);imitation learning;maximum entropy deep reinforcement learning (RL);policy gradient;value function},
  doi={10.1109/TNNLS.2022.3207346}}


@misc{wong2022deep,
      title={Deep Multiagent Reinforcement Learning: Challenges and Directions}, 
      author={Annie Wong and Thomas Bäck and Anna V. Kononova and Aske Plaat},
      year={2022},
      eprint={2106.15691},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2106.15691}, 
}

@misc{terry2023revisiting,
      title={Revisiting Parameter Sharing in Multi-Agent Deep Reinforcement Learning}, 
      author={J. K. Terry and Nathaniel Grammel and Sanghyun Son and Benjamin Black and Aakriti Agrawal},
      year={2023},
      eprint={2005.13625},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2005.13625}, 
}

@misc{pina2024fully,
      title={Fully Independent Communication in Multi-Agent Reinforcement Learning}, 
      author={Rafael Pina and Varuna De Silva and Corentin Artaud and Xiaolan Liu},
      year={2024},
      eprint={2401.15059},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2401.15059}, 
}

@misc{boldt2024review,
  title = {A {{Review}} of the {{Applications}} of {{Deep Learning-Based Emergent Communication}}},
  author = {Boldt, Brendon and Mortensen, David},
  year = {2024},
  month = jul,
  number = {arXiv:2407.03302},
  eprint = {2407.03302},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-10-01},
  abstract = {Emergent communication, or emergent language, is the field of research which studies how human language-like communication systems emerge de novo in deep multi-agent reinforcement learning environments. The possibilities of replicating the emergence of a complex behavior like language have strong intuitive appeal, yet it is necessary to complement this with clear notions of how such research can be applicable to other fields of science, technology, and engineering. This paper comprehensively reviews the applications of emergent communication research across machine learning, natural language processing, linguistics, and cognitive science. Each application is illustrated with a description of its scope, an explication of emergent communication's unique role in addressing it, a summary of the extant literature working towards the application, and brief recommendations for near-term research directions.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,I.2.7,I.6.m},
  file = {/Users/jackmontgomery/Zotero/storage/J39WZJMZ/Boldt and Mortensen - 2024 - A Review of the Applications of Deep Learning-Based Emergent Communication.pdf}
}
